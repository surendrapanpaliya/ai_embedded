{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Embedded Systems\n",
    "\n",
    "    Deep learning is a subset of machine learning that uses neural networks with multiple layers to model complex patterns in data. \n",
    "\n",
    "    When applied to embedded systems, deep learning enables these resource-constrained devices to perform tasks such as image recognition, speech processing, and sensor data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of Neural Networks\n",
    "\n",
    "    A neural network is composed of layers of interconnected nodes, called neurons, that process input data and learn patterns through training. \n",
    "    \n",
    "    These networks are inspired by the structure of the human brain and are capable of learning complex representations from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Components of Neural Networks:\n",
    "\n",
    "    1. Neurons: The fundamental units of a neural network. Each neuron receives inputs, processes them using weights, applies an activation function, and produces an output.\n",
    "\n",
    "    2. Layers: Neural networks consist of multiple layers of neurons:\n",
    "\n",
    "    Input Layer: Takes in the raw data features.\n",
    "\n",
    "    Hidden Layers: Intermediate layers where computations are performed. The network can have multiple hidden layers, leading to a \"deep\" neural network.\n",
    "\n",
    "    Output Layer: Produces the final prediction or classification result.\n",
    "\n",
    "    3. Activation Functions: Activation functions introduce non-linearity into the network, enabling it to learn complex patterns. Common activation functions include:\n",
    "\n",
    "    Sigmoid: Maps input values to the range (0, 1).\n",
    "\n",
    "    Tanh (Hyperbolic Tangent): Maps input values to the range (-1, 1).\n",
    "\n",
    "    ReLU (Rectified Linear Unit): Outputs 0 for negative inputs and the input itself for positive inputs.\n",
    "\n",
    "    Softmax: Used in the output layer for multi-class classification, producing probabilities for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Building a Simple Neural Network with TensorFlow\n",
    "\n",
    "    we'll build a simple neural network for a binary classification task using TensorFlow.\n",
    "\n",
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/surendra/anaconda3/lib/python3.11/site-packages (2.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.models\n",
    "\n",
    "#help(tensorflow.keras.models.Sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Create a Synthetic Dataset\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = np.random.rand(1000, 2)  # 1000 samples, 2 features\n",
    "\n",
    "y = (X[:, 0] + X[:, 1] > 1).astype(int)  # Binary target (0 or 1) based on a threshold\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Define the Neural Network Model\n",
    "\n",
    "# Define a simple neural network model\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(16, input_shape=(2,), activation='relu'),\n",
    "    # Input layer with 2 features, 16 neurons, ReLU activation\n",
    "    Dense(8, activation='relu'),          # Hidden layer with 8 neurons, ReLU activation\n",
    "    Dense(1, activation='sigmoid')\n",
    "    # Output layer with 1 neuron, Sigmoid activation (for binary classification)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5353 - loss: 0.6927 - val_accuracy: 0.6143 - val_loss: 0.6731\n",
      "Epoch 2/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 939us/step - accuracy: 0.6105 - loss: 0.6716 - val_accuracy: 0.6857 - val_loss: 0.6519\n",
      "Epoch 3/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 865us/step - accuracy: 0.6329 - loss: 0.6517 - val_accuracy: 0.6929 - val_loss: 0.6289\n",
      "Epoch 4/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7052 - loss: 0.6301 - val_accuracy: 0.7357 - val_loss: 0.6045\n",
      "Epoch 5/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 825us/step - accuracy: 0.7491 - loss: 0.5997 - val_accuracy: 0.7857 - val_loss: 0.5742\n",
      "Epoch 6/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 819us/step - accuracy: 0.7434 - loss: 0.5849 - val_accuracy: 0.8214 - val_loss: 0.5405\n",
      "Epoch 7/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - accuracy: 0.7920 - loss: 0.5501 - val_accuracy: 0.8214 - val_loss: 0.5014\n",
      "Epoch 8/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 921us/step - accuracy: 0.7941 - loss: 0.5136 - val_accuracy: 0.8786 - val_loss: 0.4653\n",
      "Epoch 9/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8617 - loss: 0.4624 - val_accuracy: 0.8929 - val_loss: 0.4253\n",
      "Epoch 10/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 972us/step - accuracy: 0.8587 - loss: 0.4309 - val_accuracy: 0.9214 - val_loss: 0.3885\n",
      "Epoch 11/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 932us/step - accuracy: 0.9080 - loss: 0.3808 - val_accuracy: 0.9500 - val_loss: 0.3551\n",
      "Epoch 12/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - accuracy: 0.9339 - loss: 0.3463 - val_accuracy: 0.9571 - val_loss: 0.3215\n",
      "Epoch 13/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 930us/step - accuracy: 0.9447 - loss: 0.3245 - val_accuracy: 0.9786 - val_loss: 0.2909\n",
      "Epoch 14/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 813us/step - accuracy: 0.9347 - loss: 0.2966 - val_accuracy: 0.9643 - val_loss: 0.2682\n",
      "Epoch 15/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 919us/step - accuracy: 0.9504 - loss: 0.2697 - val_accuracy: 0.9714 - val_loss: 0.2448\n",
      "Epoch 16/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step - accuracy: 0.9672 - loss: 0.2440 - val_accuracy: 0.9786 - val_loss: 0.2223\n",
      "Epoch 17/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 892us/step - accuracy: 0.9579 - loss: 0.2456 - val_accuracy: 0.9786 - val_loss: 0.2042\n",
      "Epoch 18/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9677 - loss: 0.2246 - val_accuracy: 0.9786 - val_loss: 0.1904\n",
      "Epoch 19/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9808 - loss: 0.1968 - val_accuracy: 0.9786 - val_loss: 0.1767\n",
      "Epoch 20/20\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 891us/step - accuracy: 0.9730 - loss: 0.1804 - val_accuracy: 0.9857 - val_loss: 0.1647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x30623b410>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Train the Model\n",
    "\n",
    "# Train the neural network model\n",
    "\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=16, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
      "Test Accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Evaluate the Model\n",
    "\n",
    "# Make predictions on the test set\n",
    "\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "\n",
    "# Calculate the accuracy\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Points for Embedded Systems:\n",
    "\n",
    "    Efficiency: Use lightweight architectures (e.g., fewer neurons and layers) to ensure the model runs efficiently on resource-constrained devices.\n",
    "\n",
    "    Optimization: Convert the model to TensorFlow Lite format and apply quantization to optimize it for deployment on embedded systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to TensorFlow Lite\n",
    "\n",
    "    TensorFlow Lite (TFLite) is a lightweight, open-source framework designed for deploying machine learning models on resource-constrained devices such as embedded systems, microcontrollers, mobile phones, and IoT devices. \n",
    "    \n",
    "    It allows you to run machine learning models efficiently on devices with limited computing power, memory, and storage.\n",
    "\n",
    "\n",
    "## Why Use TensorFlow Lite for Embedded Systems?\n",
    "\n",
    "    Lightweight and Optimized: TFLite models are smaller in size and optimized for inference on edge devices.\n",
    "\n",
    "    Fast Inference: Designed to perform low-latency inference, making it suitable for real-time applications.\n",
    "\n",
    "    Supports Quantization: Allows conversion of models to 8-bit integers, significantly reducing memory usage and increasing inference speed.\n",
    "\n",
    "    Runs on Various Platforms: Supports a wide range of embedded hardware, including Raspberry Pi, Arduino, Android, iOS, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tflite-runtime (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tflite-runtime\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tflite-runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Building and Training a Neural Network Model\n",
    "\n",
    "    We'll build a simple neural network using TensorFlow/Keras on a desktop or laptop. \n",
    "    \n",
    "    This model will be trained for a binary classification task, and then we will convert it to the TensorFlow Lite format for deployment on an embedded system.\n",
    "\n",
    "\n",
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Create and Train the Neural Network Model\n",
    "\n",
    "    We will use a synthetic dataset for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5404 - loss: 0.6902 - val_accuracy: 0.6375 - val_loss: 0.6733\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 855us/step - accuracy: 0.5736 - loss: 0.6736 - val_accuracy: 0.5875 - val_loss: 0.6515\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - accuracy: 0.5444 - loss: 0.6544 - val_accuracy: 0.6062 - val_loss: 0.6261\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 939us/step - accuracy: 0.5494 - loss: 0.6418 - val_accuracy: 0.6500 - val_loss: 0.6014\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782us/step - accuracy: 0.6006 - loss: 0.6204 - val_accuracy: 0.6687 - val_loss: 0.5774\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step - accuracy: 0.6751 - loss: 0.5905 - val_accuracy: 0.6750 - val_loss: 0.5506\n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 703us/step - accuracy: 0.7053 - loss: 0.5786 - val_accuracy: 0.7437 - val_loss: 0.5272\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - accuracy: 0.7558 - loss: 0.5329 - val_accuracy: 0.7937 - val_loss: 0.5024\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - accuracy: 0.8055 - loss: 0.5075 - val_accuracy: 0.8375 - val_loss: 0.4763\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782us/step - accuracy: 0.8090 - loss: 0.4884 - val_accuracy: 0.8750 - val_loss: 0.4489\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x174f1a290>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a synthetic dataset\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = np.random.rand(1000, 2)  # 1000 samples, 2 features\n",
    "\n",
    "y = (X[:, 0] + X[:, 1] > 1).astype(int)  # Binary target (0 or 1) based on a threshold\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a simple neural network model\n",
    "model = Sequential([\n",
    "    Dense(16, input_shape=(2,), activation='relu'),  # Input layer with 2 features, 16 neurons\n",
    "    Dense(8, activation='relu'),                     # Hidden layer with 8 neurons\n",
    "    Dense(1, activation='sigmoid')                   # Output layer with 1 neuron (for binary classification)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=16, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Converting the Model to TensorFlow Lite Format\n",
    "\n",
    "    Once the model is trained, convert it to TensorFlow Lite format.\n",
    "\n",
    "## Step 3: Convert the Trained Model to TensorFlow Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/j9/t7f_l7rd20101rcpcynrvqyh0000gn/T/tmp9pd716bp/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/j9/t7f_l7rd20101rcpcynrvqyh0000gn/T/tmp9pd716bp/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/var/folders/j9/t7f_l7rd20101rcpcynrvqyh0000gn/T/tmp9pd716bp'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 2), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  5981048272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6262326672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6262327824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6262328784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6262328016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6262326864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Model has been successfully converted to TensorFlow Lite format.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1730119873.990508  562879 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1730119873.990997  562879 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2024-10-28 18:21:13.991767: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/j9/t7f_l7rd20101rcpcynrvqyh0000gn/T/tmp9pd716bp\n",
      "2024-10-28 18:21:13.992093: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2024-10-28 18:21:13.992099: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /var/folders/j9/t7f_l7rd20101rcpcynrvqyh0000gn/T/tmp9pd716bp\n",
      "I0000 00:00:1730119873.995480  562879 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\n",
      "2024-10-28 18:21:13.995927: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2024-10-28 18:21:14.017234: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /var/folders/j9/t7f_l7rd20101rcpcynrvqyh0000gn/T/tmp9pd716bp\n",
      "2024-10-28 18:21:14.022492: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 30726 microseconds.\n",
      "2024-10-28 18:21:14.045797: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "# Convert the trained Keras model to TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TensorFlow Lite model to a file\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"Model has been successfully converted to TensorFlow Lite format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Apply Quantization to Optimize the Model\n",
    "\n",
    "## Quantization reduces the model's size and increases inference speed, making it more suitable for embedded hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/j9/t7f_l7rd20101rcpcynrvqyh0000gn/T/tmpxn5of50z/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/j9/t7f_l7rd20101rcpcynrvqyh0000gn/T/tmpxn5of50z/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/var/folders/j9/t7f_l7rd20101rcpcynrvqyh0000gn/T/tmpxn5of50z'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 2), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  5981048272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6262326672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6262327824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6262328784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6262328016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6262326864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Model has been successfully quantized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1730119927.782133  562879 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1730119927.782144  562879 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2024-10-28 18:22:07.782295: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/j9/t7f_l7rd20101rcpcynrvqyh0000gn/T/tmpxn5of50z\n",
      "2024-10-28 18:22:07.782606: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2024-10-28 18:22:07.782611: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /var/folders/j9/t7f_l7rd20101rcpcynrvqyh0000gn/T/tmpxn5of50z\n",
      "2024-10-28 18:22:07.785353: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2024-10-28 18:22:07.802479: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /var/folders/j9/t7f_l7rd20101rcpcynrvqyh0000gn/T/tmpxn5of50z\n",
      "2024-10-28 18:22:07.807859: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 25564 microseconds.\n"
     ]
    }
   ],
   "source": [
    "# Apply post-training quantization\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quantized_model = converter.convert()\n",
    "\n",
    "# Save the quantized model\n",
    "with open('model_quantized.tflite', 'wb') as f:\n",
    "    f.write(tflite_quantized_model)\n",
    "\n",
    "print(\"Model has been successfully quantized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Deploying and Running the Model on an Embedded System\n",
    "\n",
    "## Step 4: Transfer the Model to the Embedded Device\n",
    "\n",
    "    Transfer the model.tflite or model_quantized.tflite file to the embedded device using scp, WinSCP, or any other file transfer method.\n",
    "\n",
    "# Example command to transfer model file to Raspberry Pi\n",
    "\n",
    "    scp model_quantized.tflite pi@raspberrypi.local:/home/pi/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Perform Inference with TensorFlow Lite on the Embedded Device\n",
    "\n",
    "Install the TensorFlow Lite runtime as shown earlier, and use the following Python code to run inference on the embedded device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output: [[0.59402907]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the TensorFlow Lite model\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=\"model_quantized.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Example input data (e.g., sensor data)\n",
    "test_input = np.array([[0.4, 0.6]], dtype=np.float32)  # Modify with real sensor data\n",
    "\n",
    "# Set the input tensor\n",
    "interpreter.set_tensor(input_details[0]['index'], test_input)\n",
    "\n",
    "# Run inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get the output tensor\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(f\"Predicted Output: {output_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Points for Embedded Systems:\n",
    "\n",
    "### Quantization: Always quantize models to reduce size and improve inference speed on embedded hardware.\n",
    "\n",
    "### Optimized Libraries: Use TensorFlow Lite runtime, which is designed for embedded and edge devices, to ensure efficient model execution.\n",
    "\n",
    "### Inference Time: Test the inference time to ensure real-time performance, especially for applications like object detection, speech recognition, or predictive maintenance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (CNNs) on Embedded Devices\n",
    "\n",
    "    Convolutional Neural Networks (CNNs) are specialized deep learning architectures designed to process grid-like data, such as images and video frames. \n",
    "    \n",
    "    They have revolutionized computer vision tasks, enabling machines to perform tasks like object detection, image classification, and facial recognition with high accuracy.\n",
    "\n",
    "    Deploying CNNs on embedded devices, such as microcontrollers, Raspberry Pi, smartphones, and edge AI devices, allows you to perform real-time inference directly on the device without relying on cloud servers, which is critical for applications requiring low latency, privacy, or offline capabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture of CNNs\n",
    "\n",
    "    A typical CNN consists of several layers, each with a specific role in feature extraction and learning patterns. The key components are:\n",
    "\n",
    "    1. Convolutional Layer:\n",
    "    Applies convolutional filters (kernels) to input data to extract features such as edges, textures, and patterns.\n",
    "\n",
    "    Output of this layer is called a feature map.\n",
    "\n",
    "    2. Activation Function:\n",
    "\n",
    "    Introduces non-linearity into the network, enabling it to learn complex patterns. The most commonly used activation function is ReLU (Rectified Linear Unit).\n",
    "\n",
    "    3. Pooling Layer:\n",
    "\n",
    "    Reduces the spatial dimensions of the feature maps (downsampling) while retaining the most important information, which helps in reducing computational complexity.\n",
    "\n",
    "    Common pooling methods include Max Pooling and Average Pooling.\n",
    "\n",
    "    4. Fully Connected (Dense) Layer:\n",
    "\n",
    "    Connects every neuron from the previous layer to the next layer, typically used for final classification or regression tasks.\n",
    "\n",
    "    5. Output Layer:\n",
    "\n",
    "    Produces the final prediction, such as class probabilities in classification tasks, often using activation functions like Softmax for multi-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Image → [Convolution + Activation] → Pooling → [Convolution + Activation] → Pooling → Fully Connected Layer → Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of CNNs in Embedded Environments\n",
    "\n",
    "    CNNs have various applications in embedded systems, including but not limited to:\n",
    "\n",
    "    Object Detection: Real-time detection of objects in images or video feeds (e.g., autonomous vehicles, security cameras, robotics).\n",
    "\n",
    "    Image Classification: Identifying the category of an image (e.g., identifying fruits or animals in images for smart farming).\n",
    "\n",
    "    Facial Recognition: Authenticating users based on facial features (e.g., door security systems, smart attendance systems).\n",
    "\n",
    "    Anomaly Detection: Identifying defects or unusual patterns in industrial or medical images (e.g., identifying faults in machinery or detecting tumors in medical imaging).\n",
    "\n",
    "    Gesture Recognition: Recognizing hand gestures or movements (e.g., smart home control systems, gaming devices)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying a Simple CNN on a Embedded Device Using TensorFlow Lite\n",
    "\n",
    "    Step 1: Building and Training the CNN Model\n",
    "\n",
    "    1.Train the CNN Model on Your Computer Using TensorFlow/Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 17ms/step - accuracy: 0.9209 - loss: 0.2550 - val_accuracy: 0.9848 - val_loss: 0.0423\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 18ms/step - accuracy: 0.9886 - loss: 0.0387 - val_accuracy: 0.9905 - val_loss: 0.0283\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 18ms/step - accuracy: 0.9924 - loss: 0.0250 - val_accuracy: 0.9888 - val_loss: 0.0337\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 18ms/step - accuracy: 0.9947 - loss: 0.0173 - val_accuracy: 0.9874 - val_loss: 0.0412\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 17ms/step - accuracy: 0.9962 - loss: 0.0120 - val_accuracy: 0.9929 - val_loss: 0.0258\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x305feb910>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load the MNIST dataset (handwritten digit classification)\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = X_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0  # Reshape and normalize\n",
    "X_test = X_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)  # One-hot encoding\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Define a simple CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/j9/t7f_l7rd20101rcpcynrvqyh0000gn/T/tmprmyj28ec/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/j9/t7f_l7rd20101rcpcynrvqyh0000gn/T/tmprmyj28ec/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/var/folders/j9/t7f_l7rd20101rcpcynrvqyh0000gn/T/tmprmyj28ec'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  12985882320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  12985883664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  12985883856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  12985884432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  12985884624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  12985885200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  12985885392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  12985885968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1730892793.238215 1007534 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1730892793.238850 1007534 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been successfully converted to TensorFlow Lite format.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 17:03:13.239465: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/j9/t7f_l7rd20101rcpcynrvqyh0000gn/T/tmprmyj28ec\n",
      "2024-11-06 17:03:13.239979: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2024-11-06 17:03:13.239985: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /var/folders/j9/t7f_l7rd20101rcpcynrvqyh0000gn/T/tmprmyj28ec\n",
      "I0000 00:00:1730892793.243984 1007534 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\n",
      "2024-11-06 17:03:13.244647: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2024-11-06 17:03:13.273639: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /var/folders/j9/t7f_l7rd20101rcpcynrvqyh0000gn/T/tmprmyj28ec\n",
      "2024-11-06 17:03:13.281210: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 41760 microseconds.\n",
      "2024-11-06 17:03:13.307303: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "# 2.Convert the Trained Model to TensorFlow Lite Format\n",
    "\n",
    "# Convert the Keras model to TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TensorFlow Lite model to a file\n",
    "with open('cnn_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"Model has been successfully converted to TensorFlow Lite format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step2:Deploying and Running the Model on Raspberry Pi\n",
    "\n",
    "    ## 1.\tInstall TensorFlow Lite Runtime on Raspberry Pi\n",
    "\n",
    "# pip install tflite-runtime\n",
    "    2.\tRun Inference Using the TensorFlow Lite Model on Raspberry Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'serving_default_keras_tensor:0', 'index': 0, 'shape': array([ 1, 28, 28,  1], dtype=int32), 'shape_signature': array([-1, 28, 28,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "[{'name': 'StatefulPartitionedCall_1:0', 'index': 17, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([-1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "Predicted Digit: 9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the TensorFlow Lite model\n",
    "interpreter =  tf.lite.Interpreter(model_path=\"cnn_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "\n",
    "print(input_details)\n",
    "\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(output_details)\n",
    "\n",
    "# Example: Load an image (28x28 grayscale) and preprocess\n",
    "\n",
    "image = Image.open('/Users/surendra/Digit_Dataset/digits_jpeg/digits_jpeg/9/img010-01016.jpeg').convert('L').resize((28, 28))\n",
    "\n",
    "#image = Image.open('/Users/surendra/Digit_Dataset/digits_jpeg/digits_jpeg/7/img008-01016.jpeg').convert('L').resize((28, 28))\n",
    "\n",
    "# /Users/surendra/Digit_Dataset/digits_jpeg/digits_jpeg/8/img009-01016.jpeg\n",
    "\n",
    "# /Users/surendra/Digit_Dataset/digits_jpeg/digits_jpeg/8/img009-01010.jpeg\n",
    "\n",
    "#/Users/surendra/Digit_Dataset/digits_jpeg/digits_jpeg/7/img008-01016.jpeg\n",
    "\n",
    "input_data = np.array(image, dtype=np.float32).reshape(1, 28, 28, 1) / 255.0\n",
    "\n",
    "# Set the input tensor\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "# Perform inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get the output tensor\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "predicted_label = np.argmax(output_data)\n",
    "\n",
    "print(f\"Predicted Digit: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "    CNNs are powerful architectures for image-related tasks and have broad applications in embedded environments.\n",
    "\n",
    "    Deploying CNNs on embedded devices involves using frameworks like TensorFlow Lite, which optimize models for resource-constrained hardware.\n",
    "\n",
    "    Optimization Techniques (quantization, pruning, and model compression) are crucial to ensure CNNs run efficiently on devices with limited computational power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Concepts: Convolution, Padding, Pooling, Max Pooling, Feature Map\n",
    "\n",
    "    In Convolutional Neural Networks (CNNs), these operations are fundamental for feature extraction and dimensionality reduction. Let's break down these concepts and provide a TensorFlow example.\n",
    "\n",
    "    Key Concepts:\n",
    "\n",
    "    Convolution:\n",
    "\n",
    "    Convolution involves applying a filter (kernel) to an input to produce a feature map.\n",
    "    Filters detect features such as edges, textures, or patterns.\n",
    "    \n",
    "    Padding:\n",
    "\n",
    "    Padding adds extra pixels around the input image to control the output size after convolution.\n",
    "\n",
    "    Types:\n",
    "\n",
    "    Same Padding: Keeps the output size the same as the input.\n",
    "\n",
    "    Valid Padding: No padding is applied; reduces the output size.\n",
    "    \n",
    "    Pooling:\n",
    "\n",
    "    Pooling reduces the spatial dimensions (width and height) of feature maps while retaining the most important information.\n",
    "\n",
    "    Common types:\n",
    "\n",
    "    Max Pooling: Takes the maximum value from a region.\n",
    "\n",
    "    Average Pooling: Takes the average value from a region.\n",
    "    \n",
    "    Feature Map:\n",
    "\n",
    "    The output of applying filters through convolutional layers, capturing the learned features of the input.\n",
    "    Example: Implementing CNN Components Using TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution Kernel:\n",
      " [[[ 0.20381188]\n",
      "  [-0.02460349]\n",
      "  [-0.29725355]]\n",
      "\n",
      " [[-0.38751325]\n",
      "  [ 0.11296785]\n",
      "  [-0.1597004 ]]\n",
      "\n",
      " [[ 0.10404176]\n",
      "  [ 0.5529591 ]\n",
      "  [ 0.19280809]]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7YAAAE5CAYAAACkg7y8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAl/0lEQVR4nO3deZRU1bk34Lehm+6mmUEUUUHjAlFxuDiiAoo4QOCKmiAEBcVrHNDrGI0aERW84nWKisYIIg4oThgxiRoVEiNOK0bjEI0DDnHiA0VAUIb9/eHqvhTdjWjEYpvnWav/qF276rynus4+51fn1K6SlFIKAAAAyFSDYhcAAAAA/wrBFgAAgKwJtgAAAGRNsAUAACBrgi0AAABZE2wBAADImmALAABA1gRbAAAAsibYAgAAkDXBdh0xadKkKCkpiWeeeabYpdQYO3ZsTJs2bY37l5SUxMiRI9deQUDNWFHX36mnnrpWlvnSSy/FueeeG7Nnz14rz/+vmD17ds36n3vuuXX2OeKII2r6FFPv3r3j6KOPrtX+xhtvxMiRI6NTp05RWVkZjRs3jq222irOPvvs+Oc//1mESr+51f0fvsr48eNj0qRJtdqr/8d13fdt6tGjR5x44olrdRnwbVt5nzBjxoxa96eUYvPNN4+SkpLo1avXd17f8OHDC/ZT5eXl0blz5xg1alQsWbJkrS23Y8eOMXz48Jrb39U4QnGVFrsA1l1jx46Ngw8+OA444IBilwKs4oYbbogtttiioG3DDTdcK8t66aWXYvTo0dGrV6/o2LHjWlnGv6pp06YxadKkOOecc6JBg//7zHbhwoVxxx13RLNmzeLTTz8tWn333ntv/PnPf47JkycXtE+fPj0OOeSQaNOmTYwcOTK23377KCkpib/97W8xceLEuP/+++PZZ58tUtXfrfHjx0ebNm0KDkYjItq1axezZs2KH/zgB2t1+eeff3706dMnjjnmmOjcufNaXRZ825o2bRoTJkyoFV5nzpwZr7/+ejRt2rQ4hUVEZWVlPPLIIxER8fHHH8eUKVPivPPOi7///e9x++23fyc1fFfjCMUl2AJkaOutt44ddtih2GX8S5YuXRolJSVRWvqv74oGDRoU119/fTz88MPRp0+fmvbbb789li9fHgcccEDcfPPN//JyvqmxY8fGwIEDo3379jVtb775ZhxyyCHRqVOnePTRR6N58+Y19+21115xwgknxD333FOMctcp5eXlscsuu6z15fTs2TM6d+4cl1xySVx33XVrfXnwbRo0aFDccsstcfXVV0ezZs1q2idMmBC77rprUT/Ya9CgQcE2vP/++8fs2bNj6tSpcemllxaMi2vLdzWOUFwuRV6HDR8+PJo0aRKvvfZa9O3bN5o0aRIbb7xxnHLKKfH555/X9Ku+vGLcuHExZsyY2GSTTaKioiJ22GGHePjhh2s9Z11nXM4999yCy/RKSkpi0aJFceONN9ZcPvJ1L2GZMWNGlJSUxK233hqnn356tGvXLpo0aRL9+/ePDz/8MBYsWBBHHXVUtGnTJtq0aROHH354LFy4sOA5rr766ujRo0e0bds2qqqqomvXrjFu3LhYunRpQb+UUowdOzY6dOhQs+4PPfRQ9OrVq1bdn376aZx66qmx6aabRqNGjaJ9+/Zx4oknxqJFi77W+sG67Pbbb49dd901qqqqokmTJrHvvvvWOvP3zDPPxCGHHBIdO3aMysrK6NixYwwePDjeeuutmj6TJk2KH/3oRxERseeee9aMB9WXc616uVe1Vbe96vHgpptuilNOOSXat28f5eXl8dprr0VExB/+8Ifo3bt3NGvWLBo3bhy77bZbrfFrdTp37hzdu3ePiRMnFrRPnDgxDjzwwILQuPJrtM8++0S7du2isrIyunTpEmeccUatsaB6LH7xxRejd+/eUVVVFeutt16MHDkyPvvss6+s7dlnn42nnnoqDj300IL2Sy+9NBYtWhTjx4+vs76SkpI48MADa63PtttuGxUVFdGqVasYOHBgvPzyy3XWu7p9x9KlS6Nt27a1aoqI+OSTT6KysjJOPvnkmra33347hg4dGm3bto3y8vLo0qVLXHLJJbFixYrVrvuq+5Zq1ZdPVl/e3rFjx3jxxRdj5syZNe+x6n1VfZcQPvbYY9G7d+9o2rRpNG7cOLp37x73339/nct59NFH45hjjok2bdpE69at48ADD4z33nuvVl2HHnpo3HrrrbFgwYLVrhesawYPHhwREVOmTKlpmz9/ftx1111xxBFH1PmY0aNHx8477xytWrWKZs2axX/8x3/EhAkTIqVU0+exxx6LsrKyWl91qd62JkyY8I3qrQ6Z1fubNR1j5s2bF8cee2y0b98+GjVqFJtttlmcddZZBcfFdalrHKken1588cUYPHhwNG/ePNZff/044ogjYv78+QWP/+STT2LEiBHRqlWraNKkSfTr1y/eeOONf+nrF3z7BNt13NKlS2PAgAHRu3fvuPfee+OII46Iyy67LC666KJafa+66qr4/e9/H5dffnncfPPN0aBBg9h///1j1qxZX3u5s2bNisrKyujbt2/MmjUrZs2aFePHj/9G63DmmWfGRx99FJMmTYpLLrkkZsyYEYMHD46DDjoomjdvHlOmTImf/exncdNNN8WZZ55Z8NjXX389hgwZEjfddFNMnz49RowYERdffHH89Kc/Leh31llnxVlnnRX77bdf3HvvvXH00UfHkUceGa+++mpBv88++yx69uwZN954Y5xwwgnxu9/9Lk4//fSYNGlSDBgwoGAwh3XZ8uXLY9myZQV/1caOHRuDBw+OLbfcMqZOnRo33XRTLFiwIPbYY4946aWXavrNnj07OnfuHJdffnk88MADcdFFF8X7778fO+64Y/y///f/IiKiX79+MXbs2Ij48oOm6vGgX79+36jun//85/H222/HtddeG/fdd1+0bds2br755thnn32iWbNmceONN8bUqVOjVatWse+++36tcDtixIiYNm1afPzxxxER8corr8Tjjz8eI0aMqLP/P/7xj+jbt29MmDAhfv/738eJJ54YU6dOjf79+9fqu3Tp0ujbt2/07t07pk2bFiNHjoxf/epXMWjQoK+sa/r06dGwYcPo0aNHQfuDDz4Y66+//hqfRbjwwgtjxIgRsdVWW8Xdd98dV1xxRTz//POx6667xj/+8Y9a9a5u31FWVhZDhw6Nu+66q9aZnClTpsSSJUvi8MMPj4iIOXPmRPfu3ePBBx+M888/P37zm9/E3nvvHaeeeuq3Nq/CPffcE5tttllsv/32Ne+x1Z2tnjlzZuy1114xf/78mDBhQkyZMiWaNm0a/fv3r/PSxiOPPDLKysri1ltvjXHjxsWMGTNi6NChtfr16tUrFi1aVOd3FWFd1qxZszj44IMLPtybMmVKNGjQoN5xavbs2fHTn/40pk6dGnfffXcceOCBcfzxx8f5559f02f33XePCy64IC655JL4zW9+ExERL774Yhx33HExdOjQesfXr1L9oeZ66623xmPMkiVLYs8994zJkyfHySefHPfff38MHTo0xo0bV+tDwK/joIMOik6dOsVdd90VZ5xxRtx6661x0kkn1dy/YsWK6N+/f82JmnvuuSd23nnn2G+//b7xMllLEuuEG264IUVEevrpp2vahg0bliIiTZ06taBv3759U+fOnWtuv/nmmyki0oYbbpgWL15c0/7pp5+mVq1apb333rvgOTt06FBr+aNGjUqrvh2qqqrSsGHD1ngdIiIdd9xxNbcfffTRFBGpf//+Bf1OPPHEFBHphBNOKGg/4IADUqtWrep9/uXLl6elS5emyZMnp4YNG6Z58+allFKaN29eKi8vT4MGDSroP2vWrBQRqWfPnjVtF154YWrQoEHB65xSSnfeeWeKiPTb3/52jdcXiqF6rKjrb+nSpentt99OpaWl6fjjjy943IIFC9IGG2yQfvzjH9f73MuWLUsLFy5MVVVV6Yorrqhpv+OOO1JEpEcffbTWYzp06FDnONGzZ8+Cba96POjRo0dBv0WLFqVWrVrVGieWL1+ett1227TTTjut5tX4v/Hv4osvTgsWLEhNmjRJV111VUoppdNOOy1tuummacWKFem4446rNcatbMWKFWnp0qVp5syZKSLSc889V3Nf9Vi88muSUkpjxoxJEZEee+yx1da4//77py222KJWe0VFRdpll11W+9hqH3/8caqsrEx9+/YtaH/77bdTeXl5GjJkSK16v2rf8fzzz6eISNddd11Bv5122il169at5vYZZ5yRIiI9+eSTBf2OOeaYVFJSkl555ZWatohIo0aNqrld174lpf97H7/55ps1bVtttVXBe6Za9f/4hhtuqGnbZZddUtu2bdOCBQtq2pYtW5a23nrrtNFGG6UVK1YULOfYY48teM5x48aliEjvv/9+QfsXX3yRSkpK0umnn16rDlgXrXz8WD3OvvDCCymllHbcccc0fPjwlFL921e16mOs8847L7Vu3bpmG0rpy/Gxb9++qUWLFumFF15IW265Zdpiiy3SwoULv7K+YcOGpaqqqrR06dK0dOnSNGfOnHTFFVekkpKStOOOO6aU1nyMufbaa+sc2y666KIUEenBBx+saVt131TXOFI9Po0bN67g+Y499thUUVFR8xrcf//9KSLSNddcU9DvwgsvrDXmUVzO2K7jSkpKap092GabbQouFax24IEHRkVFRc3t6k+v//jHP8by5cvXeq31+eEPf1hwu0uXLhERtc74dOnSJebNm1dwOfKzzz4bAwYMiNatW0fDhg2jrKwsDjvssFi+fHnN2dgnnngiPv/88/jxj39c8Hy77LJLrcuup0+fHltvvXVst912BWe69t1333pnFIR10eTJk+Ppp58u+CstLY0HHnggli1bFocddljBe7yioiJ69uxZ8B5fuHBhnH766bH55ptHaWlplJaWRpMmTWLRokW1Lm/9thx00EEFtx9//PGYN29eDBs2rKDeFStWxH777RdPP/30Gn9NoEmTJvGjH/0oJk6cGMuWLYvJkyfH4YcfXu9syG+88UYMGTIkNthgg5rxpWfPnhERda7/T37yk4LbQ4YMiYiIRx99dLV1vffee9G2bds1Wof6zJo1KxYvXlzrsu+NN9449tprr1pnttdk39G1a9fo1q1b3HDDDTVtL7/8cjz11FMFly4+8sgjseWWW8ZOO+1U8HzDhw+PlFLNpDDflUWLFsWTTz4ZBx98cDRp0qSmvWHDhnHooYfGu+++G6+88krBYwYMGFBwe5tttomIqLUvLSsrixYtWmQ3GzVEfPk98R/84AcxceLE+Nvf/hZPP/10vZchR3y5be+9997RvHnzmjHwnHPOiblz58ZHH31U06+kpCQmT54cTZs2jR122CHefPPNmDp1alRVVa1RXYsWLYqysrIoKyuL9dZbL0488cTYf//9a67KWNMx5pFHHomqqqo4+OCDa/WLiK91hc/K6hoflixZUvMazJw5MyKi1nFm9eXfrDtMHrWOa9y4cUFYjfjyC/B1TZG+wQYb1Nn2xRdfxMKFC+v8Dtd3oVWrVgW3GzVqtNr2JUuWRJMmTeLtt9+OPfbYIzp37hxXXHFFdOzYMSoqKuKpp56K4447LhYvXhwREXPnzo2IiPXXX7/Wsldt+/DDD+O1116LsrKyOmutvvwS1nVdunSpc/KoDz/8MCIidtxxxzoft/KMwUOGDImHH344fvGLX8SOO+4YzZo1i5KSkujbt2/N9vVta9euXZ31rnqgsrJ58+at8QHUiBEjYvfdd48xY8bEnDlz6vz+b8SXoX6PPfaIioqKuOCCC6JTp07RuHHjeOedd+LAAw+stf6lpaXRunXrgrbqMbd6DKrP4sWL6xyfNtlkk3jzzTfXaL2ql7Hq6xfx5WzYDz30UEHbmu47jjjiiDjuuOPi73//e2yxxRZxww03RHl5ecEB29y5c+ucm6F6Fu6vWv9v28cffxwppXpfi7pqWvV/V15eHhFR5/u8oqJirb3/YW0qKSmJww8/PH75y1/GkiVLolOnTrHHHnvU2fepp56KffbZJ3r16hW//vWvY6ONNopGjRrFtGnTYsyYMbW2gdatW8eAAQPi6quvjoEDB0bXrl3XuK7Kysr44x//GBFfbnsdOnQomOBqTceYuXPnxgYbbFDrw8q2bdtGaWnpNx6Lvmp8mDt3bpSWltY6bq1rXKe4BNvvkQ8++KDOtkaNGtV8ql1RUVHnF+zXxUA3bdq0WLRoUdx9993RoUOHmva//vWvBf2qB6TqA+SVffDBBwWDZZs2baKysrLWBDMr3w85q34P33nnnQXbzarmz58f06dPj1GjRsUZZ5xR0/7555/HvHnz1nh5qxtT6tqeVj0gqe5z5ZVX1vtd069z8LDbbrtF586d47zzzos+ffrExhtvXGe/Rx55JN57772YMWNGzVnaiC8nCKnLsmXLYu7cuQUHQNVj7qoHRatq06ZNna/pvvvuG1deeWU88cQTX/k92+plvP/++7Xue++9977x2DV48OA4+eSTY9KkSTFmzJi46aab4oADDoiWLVsWLLu+5UasftysDteff/55zcFixL+2z2nZsmU0aNDgG9f0VT7++GP7ArI1fPjwOOecc+Laa6+NMWPG1Nvvtttui7Kyspg+fXrBh2DTpk2rs/9DDz0U11xzTey0005xzz33xF133VXrCpz6NGjQYLWz+K/pGNO6det48sknI6VUsC/56KOPYtmyZWttu23dunUsW7Ys5s2bVxBu6zruprhcivw9cvfddxd8Gr9gwYK47777Yo899oiGDRtGxJczT3700UcFIfCLL76IBx54oNbzlZeXF/VT6+pBa+WDoZRS/PrXvy7ot/POO0d5eXmtCUOeeOKJWpeZ/fCHP4zXX389WrduHTvssEOtv3X1NzphTe27775RWloar7/+ep3v8eqDi5KSkkgpFWxfERHXX399ra8urO7sVseOHeP5558vaHv11VdrXQpan9122y1atGgRL730Ur31Vl/NsabOPvvs6N+/f5xyyin19qlrfImI+NWvflXvY2655ZaC27feemtExFfOGL/FFlvEG2+8Uav9pJNOiqqqqjj22GNrzcAZ8eV4V32p3q677hqVlZW1frLo3XffjUceeSR69+692hrq07JlyzjggANi8uTJMX369Pjggw9qXbrYu3fveOmll+Ivf/lLQfvkyZOjpKQk9txzz3qfv3pMXfU9ct9999Xqu6b7nKqqqth5553j7rvvLui/YsWKuPnmm2OjjTaKTp06feXz1OW9996LJUuWxJZbbvmNHg/F1r59+zjttNOif//+MWzYsHr7Vf/UWvXxYcSXY/xNN91Uq+/7778fQ4cOjZ49e8bjjz8eAwYMiBEjRqzxFSdfZU3HmN69e8fChQtrhe/q3wf/puPgV6n+8HPV48zbbrttrSyPb84Z2++Rhg0bRp8+feLkk0+OFStWxEUXXRSffvppjB49uqbPoEGD4pxzzolDDjkkTjvttFiyZEn88pe/rPM7uF27do0ZM2bEfffdF+3atYumTZt+pz9a36dPn2jUqFEMHjw4fvazn8WSJUvimmuuqZnxtFqrVq3i5JNPjgsvvDBatmwZAwcOjHfffTdGjx4d7dq1K7j08sQTT4y77rorevToESeddFJss802sWLFinj77bfjwQcfjFNOOSV23nnn72wd4dvWsWPHOO+88+Kss86KN954I/bbb79o2bJlfPjhh/HUU09FVVVVjB49Opo1axY9evSIiy++ONq0aRMdO3aMmTNnxoQJE6JFixYFz7n11ltHRMR1110XTZs2jYqKith0002jdevWceihh8bQoUPj2GOPjYMOOijeeuutGDduXKy33nprVG+TJk3iyiuvjGHDhsW8efPi4IMPjrZt28acOXPiueeeizlz5sQ111zztV6DoUOH1jnj7cq6d+8eLVu2jKOPPjpGjRoVZWVlccstt8Rzzz1XZ/9GjRrFJZdcEgsXLowdd9wxHn/88bjgggti//33j9133321y+rVq1dMnDgxXn311YLAtemmm8Ztt90WgwYNiu222y5GjhwZ22+/fUREvPTSSzFx4sRIKcXAgQOjRYsW8Ytf/CLOPPPMOOyww2Lw4MExd+7cGD16dFRUVMSoUaO+1mu0siOOOCJuv/32GDlyZGy00Uax9957F9x/0kknxeTJk6Nfv35x3nnnRYcOHeL++++P8ePHxzHHHLPaENm3b99o1apVjBgxIs4777woLS2NSZMmxTvvvFOrb9euXeO2226L22+/PTbbbLOoqKio93LHCy+8MPr06RN77rlnnHrqqdGoUaMYP358vPDCCzFlypR6v1f9VZ544omIiNWGdVjX/c///M9X9unXr19ceumlMWTIkDjqqKNi7ty58b//+7+1Puxbvnx5DB48uObnGxs2bBiTJk2K7bbbLgYNGhSPPfbY1/7wcVVrOsYcdthhcfXVV8ewYcNi9uzZ0bVr13jsscdi7Nix0bdv31pj17dlv/32i9122y1OOeWU+PTTT6Nbt24xa9asmkC98nEmRVa8eatYWX2zIldVVdXqu+osk9UzvV100UVp9OjRaaONNkqNGjVK22+/fXrggQdqPf63v/1t2m677VJlZWXabLPN0lVXXVXnzJV//etf02677ZYaN25ca3bhukQ9syLfcccdX7muK6/XnDlzatruu+++tO2226aKiorUvn37dNppp6Xf/e53tWZoXbFiRbrgggtq1n2bbbZJ06dPT9tuu20aOHBgwXIWLlyYzj777NS5c+fUqFGj1Lx589S1a9d00kknpQ8++GC16wjFVt/2s6pp06alPffcMzVr1iyVl5enDh06pIMPPjj94Q9/qOnz7rvvpoMOOii1bNkyNW3aNO23337phRdeqHOm48svvzxtuummqWHDhgUzS65YsSKNGzcubbbZZqmioiLtsMMO6ZFHHql3VuRVx4NqM2fOTP369UutWrVKZWVlqX379qlfv3719q+28qzIq1PXrMiPP/542nXXXVPjxo3Teuutl4488sj0l7/8pdbMmdVj8fPPP5969eqVKisrU6tWrdIxxxyzRrOCzp8/PzVp0qTWzJvVXn/99XTsscemzTffPJWXl6fKysq05ZZbppNPPrlg1uCUUrr++uvTNttsUzN2/ed//md68cUXC/qs6b6j2vLly9PGG2+cIiKdddZZddb41ltvpSFDhqTWrVunsrKy1Llz53TxxRen5cuXF/SLOmYIfeqpp1L37t1TVVVVat++fRo1alS6/vrra82KPHv27LTPPvukpk2bpoiomcG/rtlMU0rpT3/6U9prr71SVVVVqqysTLvssku67777CvrUt71Uvx9Xnen70EMPTV27dq3zNYB10ZruE+qaFXnixImpc+fOqby8PG222WbpwgsvTBMmTCjYNs8666zUoEGD9PDDDxc89vHHH0+lpaXpv//7v1e73PrGo1Wt6Rgzd+7cdPTRR6d27dql0tLS1KFDh/Tzn/88LVmypKDf15kVeeXjzpTqnrV93rx56fDDD08tWrRIjRs3Tn369ElPPPFEnTPmUzwlKfnhztzNnj07Nt1007j44otr/YD2v7M333wztthiixg1alSt38cFWFPDhw+PO++8s2DG9q/r+OOPj4cffjhefPHFb3w2kbXr008/jQ033DAuu+yy+K//+q9ilwOs42699db4yU9+En/+85+je/fuxS6HcCky3xPPPfdcTJkyJbp37x7NmjWLV155JcaNGxfNmjX7xj8eDvBtOfvss2Py5Mlx1113rXYGaIrnsssui0022SQOP/zwYpcCrGOmTJkS//znP6Nr167RoEGDeOKJJ+Liiy+OHj16CLXrEMGW74Wqqqp45plnYsKECfHJJ59E8+bNo1evXjFmzBjTsQNFt/7668ctt9xSa44A1h3NmjWLSZMmRWmpQyOgUNOmTeO2226LCy64IBYtWhTt2rWL4cOHxwUXXFDs0liJS5EBAADImmm8AAAAyJpgCwAAQNYEWwAAALK2xjMk+HmCb6Zbt27FLqFeRx11VLFLWK11+bVbl2vju3fdddcVu4R6LV68uNgl1GujjTYqdgn16tKlS7FLWK2ysrJil1Cv5s2bF7uEes2dO7fYJdRrXX/PfVOOH787M2bMKHYJ/1Z69uxZ7BJYhTO2AAAAZE2wBQAAIGuCLQAAAFkTbAEAAMiaYAsAAEDWBFsAAACyJtgCAACQNcEWAACArAm2AAAAZE2wBQAAIGuCLQAAAFkTbAEAAMiaYAsAAEDWBFsAAACyJtgCAACQNcEWAACArAm2AAAAZE2wBQAAIGuCLQAAAFkTbAEAAMiaYAsAAEDWBFsAAACyJtgCAACQNcEWAACArAm2AAAAZE2wBQAAIGuCLQAAAFkTbAEAAMiaYAsAAEDWBFsAAACyJtgCAACQNcEWAACArAm2AAAAZE2wBQAAIGuCLQAAAFkTbAEAAMiaYAsAAEDWBFsAAACyVlrsAgByt3DhwmKXUK8vvvii2CXUa+7cucUuoV4vv/xysUtYrdLSdXf33bZt22KXUK9dd9212CUAsJY4YwsAAEDWBFsAAACyJtgCAACQNcEWAACArAm2AAAAZE2wBQAAIGuCLQAAAFkTbAEAAMiaYAsAAEDWBFsAAACyJtgCAACQNcEWAACArAm2AAAAZE2wBQAAIGuCLQAAAFkTbAEAAMiaYAsAAEDWBFsAAACyJtgCAACQNcEWAACArAm2AAAAZE2wBQAAIGuCLQAAAFkTbAEAAMiaYAsAAEDWBFsAAACyJtgCAACQNcEWAACArAm2AAAAZE2wBQAAIGuCLQAAAFkTbAEAAMiaYAsAAEDWBFsAAACyJtgCAACQNcEWAACArAm2AAAAZE2wBQAAIGulxS7g+65bt27FLgFYyxo1alTsEupVWVlZ7BLq1aJFi2KXUK91+X8aEVFSUlLsEgBgneKMLQAAAFkTbAEAAMiaYAsAAEDWBFsAAACyJtgCAACQNcEWAACArAm2AAAAZE2wBQAAIGuCLQAAAFkTbAEAAMiaYAsAAEDWBFsAAACyVlrsAgAA+H7abbfdil3Cv43XXnut2CX8W/F6f3dGjBixRv2csQUAACBrgi0AAABZE2wBAADImmALAABA1gRbAAAAsibYAgAAkDXBFgAAgKwJtgAAAGRNsAUAACBrgi0AAABZE2wBAADImmALAABA1gRbAAAAsibYAgAAkDXBFgAAgKwJtgAAAGRNsAUAACBrgi0AAABZE2wBAADImmALAABA1gRbAAAAsibYAgAAkDXBFgAAgKwJtgAAAGRNsAUAACBrpcUu4NvQrVu3YpeQJa8bfDsaNmxY7BLq1bZt22KXkKUNNtig2CWs1vz584tdQr1KS78XhxYAZMYZWwAAALIm2AIAAJA1wRYAAICsCbYAAABkTbAFAAAga4ItAAAAWRNsAQAAyJpgCwAAQNYEWwAAALIm2AIAAJA1wRYAAICsCbYAAABkTbAFAAAga4ItAAAAWRNsAQAAyJpgCwAAQNYEWwAAALIm2AIAAJA1wRYAAICsCbYAAABkTbAFAAAga4ItAAAAWRNsAQAAyJpgCwAAQNYEWwAAALIm2AIAAJA1wRYAAICsCbYAAABkTbAFAAAga4ItAAAAWRNsAQAAyJpgCwAAQNYEWwAAALIm2AIAAJA1wRYAAICsCbYAAABkTbAFAAAga4ItAAAAWSstdgHfhm7duhW7hHqty7Wt67x25GLu3LnFLqFerVu3LnYJ9Vq0aFGxS6jXhx9+WOwSVqtNmzbFLqFe8+fPL3YJAPwbcsYWAACArAm2AAAAZE2wBQAAIGuCLQAAAFkTbAEAAMiaYAsAAEDWBFsAAACyJtgCAACQNcEWAACArAm2AAAAZE2wBQAAIGuCLQAAAFkTbAEAAMiaYAsAAEDWBFsAAACyJtgCAACQNcEWAACArAm2AAAAZE2wBQAAIGuCLQAAAFkTbAEAAMiaYAsAAEDWBFsAAACyJtgCAACQNcEWAACArJUWuwAAAL6fFi9eXOwS/m088MADxS7h38oee+xR7BJYhTO2AAAAZE2wBQAAIGuCLQAAAFkTbAEAAMiaYAsAAEDWBFsAAACyJtgCAACQNcEWAACArAm2AAAAZE2wBQAAIGuCLQAAAFkTbAEAAMiaYAsAAEDWSotdAEDuPvvss2KXUK+FCxcWu4R6VVRUFLuEei1fvrzYJaxWSqnYJdRrXf6/AvD95YwtAAAAWRNsAQAAyJpgCwAAQNYEWwAAALIm2AIAAJA1wRYAAICsCbYAAABkTbAFAAAga4ItAAAAWRNsAQAAyJpgCwAAQNYEWwAAALIm2AIAAJA1wRYAAICsCbYAAABkTbAFAAAga4ItAAAAWRNsAQAAyJpgCwAAQNYEWwAAALIm2AIAAJA1wRYAAICsCbYAAABkTbAFAAAga4ItAAAAWRNsAQAAyJpgCwAAQNYEWwAAALIm2AIAAJA1wRYAAICsCbYAAABkTbAFAAAga4ItAAAAWRNsAQAAyJpgCwAAQNYEWwAAALIm2AIAAJA1wRYAAICslRa7AIDcvfDCC8UuoV5/+tOfil1Cvd55551il1CvOXPmFLuE1Vq+fHmxS6jX559/XuwS6nXMMccUu4R6jR8/vtglAGTNGVsAAACyJtgCAACQNcEWAACArAm2AAAAZE2wBQAAIGuCLQAAAFkTbAEAAMiaYAsAAEDWBFsAAACyJtgCAACQNcEWAACArAm2AAAAZE2wBQAAIGuCLQAAAFkTbAEAAMiaYAsAAEDWBFsAAACyJtgCAACQNcEWAACArAm2AAAAZE2wBQAAIGuCLQAAAFkTbAEAAMiaYAsAAEDWBFsAAACyJtgCAACQNcEWAACArAm2AAAAZE2wBQAAIGuCLQAAAFkTbAEAAMiaYAsAAEDWBFsAAACyJtgCAACQNcEWAACArAm2AAAAZE2wBQAAIGuCLQAAAFkrSSmlYhcBAAAA35QztgAAAGRNsAUAACBrgi0AAABZE2wBAADImmALAABA1gRbAAAAsibYAgAAkDXBFgAAgKwJtgAAAGTt/wPxkJPgms1FjAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1200x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a simple input image (7x7 grayscale image)\n",
    "\n",
    "input_image = np.array([\n",
    "    [1, 1, 2, 4, 5, 6, 6],\n",
    "    [1, 1, 2, 4, 5, 6, 6],\n",
    "    [1, 1, 3, 4, 5, 6, 6],\n",
    "    [1, 2, 3, 5, 5, 6, 6],\n",
    "    [1, 3, 4, 5, 6, 6, 6],\n",
    "    [1, 3, 4, 5, 6, 6, 6],\n",
    "    [1, 3, 4, 5, 6, 6, 6]\n",
    "], dtype=np.float32)\n",
    "\n",
    "input_image = input_image.reshape((1, 7, 7, 1))  # Reshape to (batch, height, width, channels)\n",
    "\n",
    "# Define a convolutional layer with a 3x3 kernel\n",
    "conv_layer = tf.keras.layers.Conv2D(\n",
    "    filters=1, kernel_size=(3, 3), strides=(1, 1), padding='same', activation=None\n",
    ")\n",
    "\n",
    "# Apply convolution\n",
    "conv_output = conv_layer(input_image)\n",
    "\n",
    "conv_weights = conv_layer.get_weights()[0]  # Get the learned kernel\n",
    "\n",
    "print(\"Convolution Kernel:\\n\", conv_weights[..., 0])\n",
    "\n",
    "# Define a max pooling layer\n",
    "max_pooling_layer = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')\n",
    "\n",
    "# Apply max pooling\n",
    "max_pooled_output = max_pooling_layer(conv_output)\n",
    "\n",
    "# Visualization of the process\n",
    "def plot_images(images, titles, cmap='gray'):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    for i, (img, title) in enumerate(zip(images, titles)):\n",
    "        plt.subplot(1, len(images), i + 1)\n",
    "        plt.title(title)\n",
    "        plt.imshow(img, cmap=cmap)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize input, feature map (convolution output), and max pooling result\n",
    "plot_images(\n",
    "    [\n",
    "        input_image[0, :, :, 0],\n",
    "        conv_output.numpy()[0, :, :, 0],\n",
    "        max_pooled_output.numpy()[0, :, :, 0]\n",
    "    ],\n",
    "    [\"Input Image\", \"Feature Map (Convolution)\", \"Max Pooling\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of the Code:\n",
    "\n",
    "    Input Image:\n",
    "\n",
    "    A small grayscale image (7x7) is created to demonstrate the operations.\n",
    "\n",
    "    Convolution:\n",
    "\n",
    "    A single filter (3x3) is applied to the input image.\n",
    "   \n",
    "    padding='same' ensures the output feature map has the same dimensions as the input.\n",
    "    \n",
    "    The learned kernel detects simple patterns or edges.\n",
    "\n",
    "    Max Pooling:\n",
    "\n",
    "    The max pooling layer with a 2x2 window and stride of 2 reduces the spatial dimensions by half.\n",
    "    \n",
    "    The output retains the most important feature values.\n",
    "\n",
    "    Visualization:\n",
    "\n",
    "    The input image, feature map after convolution, and max pooled output are plotted for better understanding.\n",
    "    Outputs:\n",
    "\n",
    "    Input Image: Original 7x7 matrix.\n",
    "\n",
    "    Feature Map: Highlights patterns detected by the convolutional kernel.\n",
    "\n",
    "    Max Pooled Output: \n",
    "    Smaller representation (3x3) that retains the strongest features from each 2x2 region.\n",
    "\n",
    "    This demonstrates how convolution, padding, and pooling interact to extract and reduce features while retaining key information for further processing in a CNN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
